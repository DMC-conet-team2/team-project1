from audio_utils import extract_audio_features
from sklearn.metrics.pairwise import cosine_similarity
from utils import OpenAIClient, OpenAIEmbedding, dump_json, read_json
from queue import Queue
from vosk import Model as VoskModel, KaldiRecognizer

import numpy as np
import json
import os
import pyloudnorm as pyln
import sounddevice as sd
import soundfile as sf
import speech_recognition as sr
import tempfile
import threading
import time
import torch
import whisper

"""
### ìŒì„± ë¶„ì„ì˜ ì •ìƒë²”ìœ„ í‰ê°€ ê¸°ì¤€
- ì†Œë¦¬ í¬ê¸° : â€“23â€¯LUFS Â± 1.0
- ë§ ë¹ ë¥´ê¸° : 129.7â€¯Â±â€¯25.9â€¯WPM WPM(â‰ˆ 1.73 ~ 2.59 WPS)

### í‰ê°€ ê¸°ì¤€ ê·¼ê±°
- ì†Œë¦¬ í¬ê¸°
    - EBU(European Broadcasting Union)ì˜ ë¼ì´ë¸Œ ë°©ì†¡ í‘œì¤€ ë²”ìœ„
    - ì°¸ê³  ê·¼ê±° : https://tech.ebu.ch/files/live/sites/tech/files/shared/r/r128.pdf
- ë§ ë¹ ë¥´ê¸°
    - (ìì—°ìŠ¤ëŸ¬ìš´ ìë°œì  ë°œí™”ì—ì„œì˜ í‰ê·  WPM ì¶”ì • ë²”ìœ„) / 60 => WPS(Words Per Second)
    - ì°¸ê³  ê·¼ê±° : https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002099342
"""

class RealTimeAudioAnalyzerWhisper:
    def __init__(self):
        self.whisper_model = self.load_whisper_model()

        self.q = Queue()
        self.sample_rate = 16000
        self.chunk_duration = 5 # ì´ˆ ë‹¨ìœ„
        self.openai_client = OpenAIClient()
        self.listening = False
        self.output_sentences = []

    def load_whisper_model(self, model_size = "large"):
        '''
        Whisper ëª¨ë¸ì„ return í•˜ëŠ” í•¨ìˆ˜(model_size ë‹¤ìš´ê·¸ë ˆì´ë“œ ì‹œ ë°œìŒì´ ë¶€ì •í™•í•  ê²½ìš° ì´ìƒí•˜ê²Œ ì¸ì‹ë˜ëŠ” ê²½ìš°ê°€ ìˆì–´ large ê³ ì •)

        íŒŒë¼ë¯¸í„°
        - model_size: Whisper ëª¨ë¸ í¬ê¸°(tiny, base, small, medium, large ì¤‘ ì„ íƒ)

        í˜¸ì¶œ ì˜ˆì‹œ: model = load_whisper_model()
        '''
        
        model = whisper.load_model(model_size) # Whisper ëª¨ë¸ ë¡œë“œ
        print(f"í˜¸ì¶œëœ ëª¨ë¸ í¬ê¸°: {model_size}")

        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª¨ë¸ì„ GPUë¡œ
        if torch.cuda.is_available():
            print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ â€” GPUë¡œ ëª¨ë¸ ë¡œë“œ ì¤‘")
            model = model.to("cuda")
        else:
            print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ â€” CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
        
        return model
    
    def audio_callback(self, indata, frames, time_info, status):
        self.q.put(indata.copy())

    def start_stream(self):
        self.listening = True
        threading.Thread(target=self._record_audio, daemon=True).start()
        threading.Thread(target=self._transcribe_loop, daemon=True).start()
        print("ğŸ§ ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤. (Ctrl+Cë¡œ ì¢…ë£Œ)")

    def _record_audio(self):
        with sd.InputStream(samplerate=self.sample_rate, channels=1, callback=self.audio_callback):
            while self.listening:
                time.sleep(0.1)

    def _transcribe_loop(self):
        while self.listening:
            audio_chunk = self.q.get()
            audio_chunk = np.squeeze(audio_chunk)

            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                filepath = f.name
                sf.write(f.name, audio_chunk, self.sample_rate)

            # Whisper ì²˜ë¦¬
            result = self.whisper_model.transcribe(
                filepath,
                fp16=torch.cuda.is_available(),
                temperature=0.2,
                initial_prompt="ì´ ì˜¤ë””ì˜¤ëŠ” ë©´ì ‘ìì˜ ë‹µë³€ì…ë‹ˆë‹¤."
            )

            text = result["text"].strip()
            duration = self.chunk_duration
            wps = len(text.split()) / duration if duration > 0 else 0

            # êµì •
            corrected = self.openai_client.create_response(
                system_content="ë©´ì ‘ìì˜ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ë©° ë§ì¶¤ë²• ìœ„ì£¼ë¡œ êµì •í•˜ë©° êµì •í•œ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
                user_content=text
            )

            # êµì • í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì €ì¥
            original_embedding = OpenAIEmbedding(text)
            corrected_embedding = OpenAIEmbedding(corrected)

            similarity = cosine_similarity([original_embedding], [corrected_embedding])[0][0]

            # ê°ì • ì¶”ì •
            features = extract_audio_features(filepath, 0, self.chunk_duration)
            emotion_prompt = f"""
            ë¬¸ì¥: "{text}"
            í‰ê·  pitch: {features["pitch_mean"]:.1f}Hz, pitch ë³€í™”ëŸ‰: {features["pitch_std"]:.2f}
            í‰ê·  ì—ë„ˆì§€: {features["energy_mean"]:.5f}, ì—ë„ˆì§€ ë³€í™”ëŸ‰: {features["energy_std"]:.5f}
            ë§ ë¹ ë¥´ê¸°(WPS): {wps:.2f}
            """
            emotion = self.openai_client.create_response(
                system_content="ì´ ë¬¸ì¥ì˜ ê°ì •ì„ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì¶”ë¡ í•˜ì„¸ìš”.",
                user_content=emotion_prompt
            )

            print(f"[{emotion}] {corrected}")

            # ì €ì¥
            self.output_sentences.append({
                "original_sentence": text,
                "corrected_sentence": corrected,
                "cosine_similarity": similarity,
                "emotion": emotion,
                "wps": wps
            })

            os.remove(filepath)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ

    def stop(self):
        self.listening = False
        dump_json(filename="realtime_output", json_data={"interview": self.output_sentences})
        print("\nğŸ›‘ ë¶„ì„ ì¢…ë£Œ. JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ.")

class RealTimeAudioAnalyzerVosk:
    def __init__(self):
        self.vosk_model = VoskModel("vosk-model-small-ko-0.22")  # í•œêµ­ì–´ ëª¨ë¸ ê²½ë¡œ
        self.recognizer = KaldiRecognizer(self.vosk_model, 16000)

        self.q = Queue()
        self.sample_rate = 16000
        self.chunk_duration = 3  # 1~3ë²ˆ: shorter for quicker response
        self.openai_client = OpenAIClient()
        self.listening = False
        self.output_sentences = []

    def audio_callback(self, indata, frames, time_info, status):
        self.q.put(indata.copy())

    def start_stream(self):
        self.listening = True
        threading.Thread(target=self._record_audio, daemon=True).start()
        threading.Thread(target=self._transcribe_loop, daemon=True).start()
        print("ğŸ§ ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤. (Ctrl+Cë¡œ ì¢…ë£Œ)")

    def _record_audio(self):
        with sd.InputStream(samplerate=self.sample_rate, channels=1, callback=self.audio_callback):
            while self.listening:
                time.sleep(0.1)

    def _transcribe_loop(self):
        buffer = b''

        while self.listening:
            chunk = self.q.get()
            buffer += chunk.tobytes()

            if self.recognizer.AcceptWaveform(buffer):
                result = json.loads(self.recognizer.Result())
                text = result.get("text", "").strip()

                if not text:
                    buffer = b''  # âš ï¸ ì´ ìœ„ì¹˜ì—ì„œ ì´ˆê¸°í™”
                    continue

                # save audio to temp file BEFORE buffer clear
                audio_array = np.frombuffer(buffer, dtype=np.int16)
                with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                    filepath = f.name
                    sf.write(filepath, audio_array, self.sample_rate)

                buffer = b''  # âœ… buffer ì´ˆê¸°í™” ìœ„ì¹˜ ì´ìª½ì´ ë§ìŒ

                # ë¶„ì„
                duration = self.chunk_duration
                wps = len(text.split()) / duration if duration > 0 else 0

                corrected = self.openai_client.create_response(
                    system_content="ë©´ì ‘ìì˜ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ë©° ë§ì¶¤ë²• ìœ„ì£¼ë¡œ êµì •í•˜ë©° êµì •í•œ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
                    user_content=text
                )

                original_embedding = OpenAIEmbedding(text)
                corrected_embedding = OpenAIEmbedding(corrected)
                similarity = cosine_similarity([original_embedding], [corrected_embedding])[0][0]

                features = extract_audio_features(filepath, 0, self.chunk_duration)
                emotion_prompt = f"""
                ë¬¸ì¥: "{text}"
                í‰ê·  pitch: {features["pitch_mean"]:.1f}Hz, pitch ë³€í™”ëŸ‰: {features["pitch_std"]:.2f}
                í‰ê·  ì—ë„ˆì§€: {features["energy_mean"]:.5f}, ì—ë„ˆì§€ ë³€í™”ëŸ‰: {features["energy_std"]:.5f}
                ë§ ë¹ ë¥´ê¸°(WPS): {wps:.2f}
                """

                emotion = self.openai_client.create_response(
                    system_content="ì´ ë¬¸ì¥ì˜ ê°ì •ì„ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì¶”ë¡ í•˜ì„¸ìš”.",
                    user_content=emotion_prompt
                )

                print(f"[{emotion}] {corrected}")

                self.output_sentences.append({
                    "original_sentence": text,
                    "corrected_sentence": corrected,
                    "cosine_similarity": similarity,
                    "emotion": emotion,
                    "wps": wps
                })

                os.remove(filepath)

    def stop(self):
        self.listening = False
        dump_json(filename="realtime_output", json_data={"interview": self.output_sentences})
        print("\nğŸ›‘ ë¶„ì„ ì¢…ë£Œ. JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ.")

class RealTimeAudioAnalyzerGoogleSTT:
    def __init__(self, use_google_cloud=True):
        self.q = Queue()
        self.sample_rate = 16000
        self.chunk_duration = 5  # ì´ˆ ë‹¨ìœ„
        self.listening = False
        self.output_sentences = []
        self.openai_client = OpenAIClient()
        self.use_google_cloud = use_google_cloud  # Trueë©´ GCP, Falseë©´ Google Web Speech API ì‚¬ìš©
        self.recognizer = sr.Recognizer()
        self.recognizer.energy_threshold = 300  # ìŒì„± ì¸ì‹ ë¯¼ê°ë„ ì„¤ì •

    def audio_callback(self, indata, frames, time_info, status):
        self.q.put(indata.copy())

    def start_stream(self):
        self.listening = True
        threading.Thread(target=self._record_audio, daemon=True).start()
        threading.Thread(target=self._transcribe_loop, daemon=True).start()
        print("ğŸ§ Google STT ê¸°ë°˜ ì‹¤ì‹œê°„ ìŒì„± ë¶„ì„ ì‹œì‘ (Ctrl+Cë¡œ ì¢…ë£Œ)")

    def _record_audio(self):
        import sounddevice as sd
        with sd.InputStream(samplerate=self.sample_rate, channels=1, callback=self.audio_callback):
            while self.listening:
                time.sleep(0.1)

    def _transcribe_loop(self):
        while self.listening:
            audio_chunk = self.q.get()
            audio_chunk = np.squeeze(audio_chunk)

            # WAVë¡œ ì €ì¥
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as f:
                filepath = f.name
                sf.write(filepath, audio_chunk, self.sample_rate)

            # Google STTë¡œ ìŒì„± ì¸ì‹
            with sr.AudioFile(filepath) as source:
                audio = self.recognizer.record(source)
                try:
                    if self.use_google_cloud:
                        text = self.recognizer.recognize_google_cloud(audio, language="ko-KR")
                    else:
                        text = self.recognizer.recognize_google(audio, language="ko-KR")
                except sr.UnknownValueError:
                    print("â— ìŒì„±ì„ ì¸ì‹í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    os.remove(filepath)
                    continue
                except sr.RequestError as e:
                    print(f"â— ìš”ì²­ ì‹¤íŒ¨: {e}")
                    os.remove(filepath)
                    continue

            text = text.strip()
            duration = self.chunk_duration
            wps = len(text.split()) / duration if duration > 0 else 0

            # êµì •
            corrected = self.openai_client.create_response(
                system_content="ë©´ì ‘ìì˜ ë§íˆ¬ë¥¼ ìœ ì§€í•˜ë©° ë§ì¶¤ë²• ìœ„ì£¼ë¡œ êµì •í•˜ë©° êµì •í•œ ë‚´ìš©ë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
                user_content=text
            )

            original_embedding = OpenAIEmbedding(text)
            corrected_embedding = OpenAIEmbedding(corrected)
            similarity = cosine_similarity([original_embedding], [corrected_embedding])[0][0]

            # ê°ì • ì¶”ì •
            features = extract_audio_features(filepath, 0, self.chunk_duration)
            emotion_prompt = f"""
            ë¬¸ì¥: "{text}"
            í‰ê·  pitch: {features["pitch_mean"]:.1f}Hz, pitch ë³€í™”ëŸ‰: {features["pitch_std"]:.2f}
            í‰ê·  ì—ë„ˆì§€: {features["energy_mean"]:.5f}, ì—ë„ˆì§€ ë³€í™”ëŸ‰: {features["energy_std"]:.5f}
            ë§ ë¹ ë¥´ê¸°(WPS): {wps:.2f}
            """
            emotion = self.openai_client.create_response(
                system_content="ì´ ë¬¸ì¥ì˜ ê°ì •ì„ í•˜ë‚˜ì˜ ë‹¨ì–´ë¡œ ì¶”ë¡ í•˜ì„¸ìš”.",
                user_content=emotion_prompt
            )

            print(f"[{emotion}] {corrected}")

            self.output_sentences.append({
                "original_sentence": text,
                "corrected_sentence": corrected,
                "cosine_similarity": similarity,
                "emotion": emotion,
                "wps": wps
            })

            os.remove(filepath)

    def stop(self):
        self.listening = False
        dump_json(filename="realtime_output", json_data={"interview": self.output_sentences})
        print("\nğŸ›‘ ë¶„ì„ ì¢…ë£Œ. JSON íŒŒì¼ ì €ì¥ ì™„ë£Œ.")

class StaticAudioAnalyzer:
    def __init__(self):
        self.whisper_model = self.load_whisper_model()
        self.openai_client = OpenAIClient()

    def load_whisper_model(self, model_size = "large"):
        '''
        Whisper ëª¨ë¸ì„ return í•˜ëŠ” í•¨ìˆ˜(model_size ë‹¤ìš´ê·¸ë ˆì´ë“œ ì‹œ ë°œìŒì´ ë¶€ì •í™•í•  ê²½ìš° ì´ìƒí•˜ê²Œ ì¸ì‹ë˜ëŠ” ê²½ìš°ê°€ ìˆì–´ large ê³ ì •)

        íŒŒë¼ë¯¸í„°
        - model_size: Whisper ëª¨ë¸ í¬ê¸°(tiny, base, small, medium, large ì¤‘ ì„ íƒ)

        í˜¸ì¶œ ì˜ˆì‹œ: model = load_whisper_model()
        '''
        
        model = whisper.load_model(model_size) # Whisper ëª¨ë¸ ë¡œë“œ
        print(f"í˜¸ì¶œëœ ëª¨ë¸ í¬ê¸°: {model_size}")

        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª¨ë¸ì„ GPUë¡œ
        if torch.cuda.is_available():
            print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ â€” GPUë¡œ ëª¨ë¸ ë¡œë“œ ì¤‘")
            model = model.to("cuda")
        else:
            print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ â€” CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
        
        return model

    def analize_audio(self, audio_dir):
        """
        ì£¼ì–´ì§„ ìŒì„± íŒŒì¼ì„ Whisperë¡œ ë¶„ì„í•˜ì—¬ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³ , ë¬¸ì¥ë³„ ì‹œì‘/ë ì‹œê°„ê³¼ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜

        ì‹¤í–‰ ì „ ì„¸íŒ…ì€ ì•„ë˜ ë§í¬ ì°¸ì¡°
        - https://velog.io/@strurao/Python-OpenAI-Whisper-%EC%9D%8C%EC%84%B1%EC%9D%B8%EC%8B%9D#-%EC%84%A4%EC%B9%98-%EA%B3%BC%EC%A0%95

        ìœ„ ë§í¬ ì„¸íŒ… ì¤‘ í•„ìˆ˜ ì—¬ë¶€ ì •ë¦¬
        - torch         | âœ… í•„ìˆ˜           | WhisperëŠ” PyTorch ê¸°ë°˜, ë°˜ë“œì‹œ ì„¤ì¹˜
        - torchaudio    | âŒ ì„ íƒ           | Whisper ê¸°ë³¸ ì‘ë™ì—ëŠ” í•„ìš” ì—†ì§€ë§Œ, ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ë“±ì„ ìœ„í•´ ìœ ìš©
        - torchvision   | âŒ ë¶ˆí•„ìš”         | ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê²ƒìœ¼ë¡œ, Whisperì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        - ffmpeg        | âœ… ì‚¬ì‹¤ìƒ í•„ìˆ˜    | WhisperëŠ” ffmpegë¥¼ ë°±ì—”ë“œë¡œ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í¬ë§·ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨

        íŒŒë¼ë¯¸í„°
        - client: OpenAI API í´ë¼ì´ì–¸íŠ¸
        - audio_dir (str): ë¶„ì„ í•  ìŒì„± íŒŒì¼ ê²½ë¡œ

        í˜¸ì¶œ ì˜ˆì‹œ: analize_audio(client, './audio/voice-sample.mp3')
        """

        # ì…ë ¥í•œ íŒŒì¼ ê²½ë¡œ ê²€ì¦
        if not audio_dir:
            print('ìŒì„± íŒŒì¼ ê²½ë¡œ ì§€ì •ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.')
            return
        if not os.path.exists(audio_dir):
            print('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìŒì„±íŒŒì¼ì…ë‹ˆë‹¤.\níŒŒì¼ ìœ„ì¹˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.')
            return

        # í‰ê·  ì†Œë¦¬ í¬ê¸° ê³„ì‚°
        data, rate = sf.read(audio_dir)
        meter = pyln.Meter(rate) 
        loudness = meter.integrated_loudness(data)
        print(f"Integrated Loudness: {loudness:.2f} LUFS")

        # transcribe()ëŠ” ì˜¤ë””ì˜¤ë¥¼ 30ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        result = self.whisper_model.transcribe(
            audio_dir,
            fp16=torch.cuda.is_available(),
            temperature=0.2, 
            initial_prompt="ì´ ì˜¤ë””ì˜¤ëŠ” ë©´ì ‘ìì˜ ë‹µë³€ì´ë©° ì´ìœ , ë™ê¸°, ë°°ê²½, ì„±ê²©, ê²½í—˜ ë“±ì˜ ë‹¨ì–´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            beam_size = 5 # beam search ì‚¬ìš© (ë” ë§ì€ í›„ë³´ íƒìƒ‰ â†’ ì •í™•ë„ í–¥ìƒ)
        ) 
        print(f"ëª¨ë¸ transcribe ì™„ë£Œ. ê°ì§€ëœ ì–¸ì–´: {result['language']}") # ê°ì§€ëœ ì–¸ì–´ ì¶œë ¥

        # ë¬¸ì¥ë³„ ì‹œê°„ ì¶œë ¥
        sentences = []
        for segment in result["segments"]:
            start = segment["start"]
            end = segment["end"]
            text = segment["text"].strip()

            # ë§ ë¹ ë¥´ê¸° ê³„ì‚°(ì´ˆ ë‹¹ ëª‡ ê°œì˜ ë‹¨ì–´ë¥¼ ë§ í–ˆëŠ”ì§€)
            duration = end - start
            word_cnt = len(text.split()) # Whisperë¡œ ì½ì–´ì˜¨ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ì¤€
            wps = word_cnt / duration if duration > 0 else 0 # words per second

            # 1. ë¶€ì •í™•í•œ ë°œìŒì´ë‚˜ ë¬¸ë§¥ ìƒ ìì—°ìŠ¤ëŸ½ì§€ ì•Šì€ í‘œí˜„ì„ GPTë¥¼ í†µí•´ í›„ì²˜ë¦¬ êµì •
            corrected = self.openai_client.create_response(
                system_content = (
                    "ë‹¹ì‹ ì€ ë©´ì ‘ìì˜ ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ ë§ì¶¤ë²•ê³¼ íë¦„ ìœ„ì£¼ë¡œ êµì •í•˜ëŠ” êµì • ë„ìš°ë¯¸ì…ë‹ˆë‹¤. \
                    ì‚¬ìš©ìì˜ ì–´íˆ¬ ë° ì–´ë¯¸ë¥¼ ë³´ì¡´í•˜ê³ , ë¬¸ë²• ì˜¤ë¥˜ì™€ ì• ë’¤ ë‹¨ì–´ì™€ ì´ì–´ì§€ì§€ ì•ŠëŠ” ì–´ìƒ‰í•œ í‘œí˜„ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”. ì˜ˆ: 'êµë³¸ ê·¼ë¬´' â†’ 'êµë²ˆê·¼ë¬´' í˜¹ì€ 'êµëŒ€ê·¼ë¬´' \
                    ì˜¤íƒ€ë‚˜ ì˜ëª» ì¸ì‹ëœ ë‹¨ì–´ëŠ” ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ë³µì›í•˜ì„¸ìš”. ì˜ˆ: 'ì´í›„' â†’ 'ì´ìœ '. \
                    ë‹µë³€ì€ ë¬¸ì¥ í•˜ë‚˜ë¡œ ì¶œë ¥í•˜ë˜, ë©´ì ‘ ë§íˆ¬(ìì—°ìŠ¤ëŸ½ê³  ê³µì†í•œ êµ¬ì–´ì²´)ë¥¼ ìœ ì§€í•˜ë©° ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë°”ê¾¸ì§€ ë§ê³  \
                    í‘œí˜„ë§Œ ìœ„ì— ì œì‹œí•œëŒ€ë¡œ ë‹¤ë“¬ìœ¼ì„¸ìš”."
                ),
                user_content = text
            )

            # êµì • í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì €ì¥
            original_embedding = OpenAIEmbedding(text)
            corrected_embedding = OpenAIEmbedding(corrected)

            similarity = cosine_similarity([original_embedding], [corrected_embedding])[0][0]

            # 2. ê°ì • ë¶„ì„
            audio_features = extract_audio_features(audio_dir, start, end) # ìŒí–¥ì  íŠ¹ì§• ì¶”ì¶œ

            emotion_prompt = f"""
            ë¬¸ì¥: "{text}"
            í‰ê·  pitch: {audio_features["pitch_mean"]:.1f}Hz, pitch ë³€í™”ëŸ‰: {audio_features["pitch_std"]:.2f}
            í‰ê·  ì—ë„ˆì§€: {audio_features["energy_mean"]:.5f}, ì—ë„ˆì§€ ë³€í™”ëŸ‰: {audio_features["energy_std"]:.5f}
            ë§ ë¹ ë¥´ê¸°(WPS): {wps:.2f}
            """

            emotion = self.openai_client.create_response(
                system_content = (
                    "ë‹¹ì‹ ì€ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "í…ìŠ¤íŠ¸ì™€ ìŒí–¥ì  íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì´ ë¬¸ì¥ì˜ ê°ì •ì„ ì¶”ë¡ í•˜ì„¸ìš”: "
                    "ê°ì • í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
                ),
                user_content = emotion_prompt
            )

            print(f"[{start:.1f}s --> {end:.1f}s] [{emotion}] {corrected}")

            # ë¶„ì„ ëœ ë¬¸ì¥ íŒŒì¼ ì €ì¥ì„ ìœ„í•´ append
            sentences.append({
                "original_sentence": text,
                "corrected_sentence": corrected,
                "cosine_similarity": similarity,
                "emotion": emotion,
                "start": start,
                "end": end,
                "wps": wps
            })

        # JSON ë°ì´í„° êµ¬ì„±
        output_data = {"interview": sentences, "LUFS": loudness}
        base_filename = os.path.splitext(os.path.basename(audio_dir))[0]

        dump_json(filename = base_filename, json_data = output_data)

class AudioEvaluator:
    def __init__(self, filename):
        self.filename = filename
        self.data = read_json(filename = filename)
    
    def evaluate(self):
        # ë¬¸ì¥ë³„ í‰ê°€
        similarities = []
        for idx, sentence in enumerate(self.data.get("interview", []), 1):
            similarity = sentence.get("cosine_similarity")
            emotion = sentence.get("emotion")
            wps = sentence.get("wps")
            
            similarities.append(similarity)
            if similarity < 0.8:
                print(f"âŒ ë¬¸ì¥ {idx}: ì˜ë¯¸ì  ì°¨ì´ í¼ (ìœ ì‚¬ë„ {similarity:.4f}) â€” êµì • í•„ìš”")
            elif similarity < 0.9:
                print(f"âš ï¸ ë¬¸ì¥ {idx}: ì•½ê°„ì˜ ì°¨ì´ ìˆìŒ (ìœ ì‚¬ë„ {similarity:.4f}) â€” ìì—°ìŠ¤ëŸ¬ì›€ ê°œì„ ")
            else:
                print(f"âœ… ë¬¸ì¥ {idx}: ê±°ì˜ ë™ì¼ (ìœ ì‚¬ë„ {similarity:.4f}) â€” ë¬¸ë²• êµì •ë§Œ í•„ìš”")

            print(f"ë¬¸ì¥ {idx}ì˜ ê°ì • : {emotion}")

            if wps is None:
                print(f"ë¬¸ì¥ {idx}: WPS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            elif 1.73 <= wps <= 2.59:
                print(f"âœ… ë¬¸ì¥ {idx}ì˜ WPS [{wps}] : ì •ìƒ")
            else:
                print(f"âš ï¸ ë¬¸ì¥ {idx}ì˜ WPS [{wps}] : ë¹ ë¥´ê±°ë‚˜ ëŠë¦½ë‹ˆë‹¤.")
    
        # ì „ì²´ í‰ê°€
        lufs = self.data.get("LUFS")
        if lufs is None:
            print("LUFS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        elif -24 <= lufs <= -22:
            print(f"âœ… ëª©ì†Œë¦¬ í‰ê·  í¬ê¸° [{lufs} LUFS] : ì •ìƒ")
        else:
            print(f"âš ï¸ ëª©ì†Œë¦¬ í‰ê·  í¬ê¸° [{lufs} LUFS] : ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìŠµë‹ˆë‹¤. ")
        
        if similarities:
            avg_sim = sum(similarities) / len(similarities)
            print(f"\nğŸ“Š ì „ì²´ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {avg_sim:.4f}")