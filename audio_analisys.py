from audio_utils import extract_audio_features
from dotenv import load_dotenv
from google.cloud import speech
from six.moves import queue
from sklearn.metrics.pairwise import cosine_similarity
from utils import OpenAIClient, OpenAIEmbedding, dump_json, read_json

import os
import pyaudio
import pyloudnorm as pyln
import soundfile as sf
import torch
import whisper

"""
### ìŒì„± ë¶„ì„ì˜ ì •ìƒë²”ìœ„ í‰ê°€ ê¸°ì¤€
- ì†Œë¦¬ í¬ê¸° : â€“23â€¯LUFS Â± 1.0
- ë§ ë¹ ë¥´ê¸° : 129.7â€¯Â±â€¯25.9â€¯WPM WPM(â‰ˆ 1.73 ~ 2.59 WPS)

### í‰ê°€ ê¸°ì¤€ ê·¼ê±°
- ì†Œë¦¬ í¬ê¸°
    - EBU(European Broadcasting Union)ì˜ ë¼ì´ë¸Œ ë°©ì†¡ í‘œì¤€ ë²”ìœ„
    - ì°¸ê³  ê·¼ê±° : https://tech.ebu.ch/files/live/sites/tech/files/shared/r/r128.pdf
- ë§ ë¹ ë¥´ê¸°
    - (ìì—°ìŠ¤ëŸ¬ìš´ ìë°œì  ë°œí™”ì—ì„œì˜ í‰ê·  WPM ì¶”ì • ë²”ìœ„) / 60 => WPS(Words Per Second)
    - ì°¸ê³  ê·¼ê±° : https://www.kci.go.kr/kciportal/ci/sereArticleSearch/ciSereArtiView.kci?sereArticleSearchBean.artiId=ART002099342
"""

# ë§ˆì´í¬ ì„¤ì •
RATE = 16000
CHUNK = int(RATE / 10)  # 100ms

class RealtimeAudioAnalyzer:
    def __init__(self, language_code="ko-KR"):
        load_dotenv()

        self.language_code = language_code
        self._buff = queue.Queue()
        self.closed = True
        self.client = speech.SpeechClient()
        self.streaming_config = self._get_streaming_config()

    def _get_streaming_config(self):
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=RATE,
            language_code=self.language_code,
            enable_automatic_punctuation=True,
        )
        return speech.StreamingRecognitionConfig(
            config=config,
            interim_results=True
        )

    def _fill_buffer(self, in_data, frame_count, time_info, status_flags):
        self._buff.put(in_data)
        return None, pyaudio.paContinue

    def _audio_generator(self):
        while not self.closed:
            chunk = self._buff.get()
            if chunk is None:
                return
            yield speech.StreamingRecognizeRequest(audio_content=chunk)

    def _listen_print_loop(self, responses):
        for response in responses:
            print("ğŸ“¥ ì‘ë‹µ ìˆ˜ì‹ ë¨")

            if not response.results:
                print("âš ï¸ ë¹ˆ ì‘ë‹µ")
                continue

            result = response.results[0]
            transcript = result.alternatives[0].transcript

            if result.is_final:
                print(f"ğŸ—£ï¸ [ìµœì¢…] ì¸ì‹ ê²°ê³¼: {transcript}")
            else:
                print(f"ğŸ’¬ [ì¤‘ê°„] {transcript}", end="\r")  # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸

    def start(self):
        print("ğŸ”§ start() ì‹¤í–‰ë¨")  # â† ì´ ì¤„ ì¶”ê°€

        self.closed = False
        audio_interface = pyaudio.PyAudio()
        audio_stream = audio_interface.open(
            format=pyaudio.paInt16,
            channels=1, rate=RATE,
            input=True, frames_per_buffer=CHUNK,
            stream_callback=self._fill_buffer,
        )

        print("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘ (ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+C)...")

        try:
            print("ğŸ” ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì‹œì‘ë¨")
            requests = self._audio_generator()
            responses = self.client.streaming_recognize(self.streaming_config, requests)
            self._listen_print_loop(responses)
        except KeyboardInterrupt:
            print("\nğŸ›‘ ì¸ì‹ ì¤‘ì§€ë¨.")
        finally:
            print("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘")
            audio_stream.stop_stream()
            audio_stream.close()
            audio_interface.terminate()
            self.closed = True
            self._buff.put(None)

class StaticAudioAnalyzer:
    def __init__(self):
        self.whisper_model = self.load_whisper_model()
        self.openai_client = OpenAIClient()

    def load_whisper_model(self, model_size = "large"):
        '''
        Whisper ëª¨ë¸ì„ return í•˜ëŠ” í•¨ìˆ˜(model_size ë‹¤ìš´ê·¸ë ˆì´ë“œ ì‹œ ë°œìŒì´ ë¶€ì •í™•í•  ê²½ìš° ì´ìƒí•˜ê²Œ ì¸ì‹ë˜ëŠ” ê²½ìš°ê°€ ìˆì–´ large ê³ ì •)

        íŒŒë¼ë¯¸í„°
        - model_size: Whisper ëª¨ë¸ í¬ê¸°(tiny, base, small, medium, large ì¤‘ ì„ íƒ)

        í˜¸ì¶œ ì˜ˆì‹œ: model = load_whisper_model()
        '''
        
        model = whisper.load_model(model_size) # Whisper ëª¨ë¸ ë¡œë“œ
        print(f"í˜¸ì¶œëœ ëª¨ë¸ í¬ê¸°: {model_size}")

        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ ëª¨ë¸ì„ GPUë¡œ
        if torch.cuda.is_available():
            print("âœ… CUDA ì‚¬ìš© ê°€ëŠ¥ â€” GPUë¡œ ëª¨ë¸ ë¡œë“œ ì¤‘")
            model = model.to("cuda")
        else:
            print("âš ï¸ CUDA ì‚¬ìš© ë¶ˆê°€ â€” CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤")
        
        return model

    def analize_audio(self, audio_dir):
        """
        ì£¼ì–´ì§„ ìŒì„± íŒŒì¼ì„ Whisperë¡œ ë¶„ì„í•˜ì—¬ ì–¸ì–´ë¥¼ ê°ì§€í•˜ê³ , ë¬¸ì¥ë³„ ì‹œì‘/ë ì‹œê°„ê³¼ í…ìŠ¤íŠ¸ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜

        ì‹¤í–‰ ì „ ì„¸íŒ…ì€ ì•„ë˜ ë§í¬ ì°¸ì¡°
        - https://velog.io/@strurao/Python-OpenAI-Whisper-%EC%9D%8C%EC%84%B1%EC%9D%B8%EC%8B%9D#-%EC%84%A4%EC%B9%98-%EA%B3%BC%EC%A0%95

        ìœ„ ë§í¬ ì„¸íŒ… ì¤‘ í•„ìˆ˜ ì—¬ë¶€ ì •ë¦¬
        - torch         | âœ… í•„ìˆ˜           | WhisperëŠ” PyTorch ê¸°ë°˜, ë°˜ë“œì‹œ ì„¤ì¹˜
        - torchaudio    | âŒ ì„ íƒ           | Whisper ê¸°ë³¸ ì‘ë™ì—ëŠ” í•„ìš” ì—†ì§€ë§Œ, ì˜¤ë””ì˜¤ ì „ì²˜ë¦¬ ë“±ì„ ìœ„í•´ ìœ ìš©
        - torchvision   | âŒ ë¶ˆí•„ìš”         | ì´ë¯¸ì§€ ì²˜ë¦¬ë¥¼ ìœ„í•œ ê²ƒìœ¼ë¡œ, Whisperì—ì„œëŠ” ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
        - ffmpeg        | âœ… ì‚¬ì‹¤ìƒ í•„ìˆ˜    | WhisperëŠ” ffmpegë¥¼ ë°±ì—”ë“œë¡œ ì‚¬ìš©í•˜ì—¬ ë‹¤ì–‘í•œ ì˜¤ë””ì˜¤ í¬ë§·ì„ ì²˜ë¦¬í•˜ë¯€ë¡œ ì‹œìŠ¤í…œì— ì„¤ì¹˜ë˜ì–´ ìˆì–´ì•¼ í•¨

        íŒŒë¼ë¯¸í„°
        - client: OpenAI API í´ë¼ì´ì–¸íŠ¸
        - audio_dir (str): ë¶„ì„ í•  ìŒì„± íŒŒì¼ ê²½ë¡œ

        í˜¸ì¶œ ì˜ˆì‹œ: analize_audio(client, './audio/voice-sample.mp3')
        """

        # ì…ë ¥í•œ íŒŒì¼ ê²½ë¡œ ê²€ì¦
        if not audio_dir:
            print('ìŒì„± íŒŒì¼ ê²½ë¡œ ì§€ì •ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤.')
            return
        if not os.path.exists(audio_dir):
            print('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìŒì„±íŒŒì¼ì…ë‹ˆë‹¤.\níŒŒì¼ ìœ„ì¹˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.')
            return

        # í‰ê·  ì†Œë¦¬ í¬ê¸° ê³„ì‚°
        data, rate = sf.read(audio_dir)
        meter = pyln.Meter(rate) 
        loudness = meter.integrated_loudness(data)
        print(f"Integrated Loudness: {loudness:.2f} LUFS")

        # transcribe()ëŠ” ì˜¤ë””ì˜¤ë¥¼ 30ì´ˆ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ìë™ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        result = self.whisper_model.transcribe(
            audio_dir,
            fp16=torch.cuda.is_available(),
            temperature=0.2, 
            initial_prompt="ì´ ì˜¤ë””ì˜¤ëŠ” ë©´ì ‘ìì˜ ë‹µë³€ì´ë©° ì´ìœ , ë™ê¸°, ë°°ê²½, ì„±ê²©, ê²½í—˜ ë“±ì˜ ë‹¨ì–´ê°€ í¬í•¨ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            beam_size = 5 # beam search ì‚¬ìš© (ë” ë§ì€ í›„ë³´ íƒìƒ‰ â†’ ì •í™•ë„ í–¥ìƒ)
        ) 
        print(f"ëª¨ë¸ transcribe ì™„ë£Œ. ê°ì§€ëœ ì–¸ì–´: {result['language']}") # ê°ì§€ëœ ì–¸ì–´ ì¶œë ¥

        # ë¬¸ì¥ë³„ ì‹œê°„ ì¶œë ¥
        sentences = []
        for segment in result["segments"]:
            start = segment["start"]
            end = segment["end"]
            text = segment["text"].strip()

            # ë§ ë¹ ë¥´ê¸° ê³„ì‚°(ì´ˆ ë‹¹ ëª‡ ê°œì˜ ë‹¨ì–´ë¥¼ ë§ í–ˆëŠ”ì§€)
            duration = end - start
            word_cnt = len(text.split()) # Whisperë¡œ ì½ì–´ì˜¨ ì›ë³¸ í…ìŠ¤íŠ¸ ê¸°ì¤€
            wps = word_cnt / duration if duration > 0 else 0 # words per second

            # 1. ë¶€ì •í™•í•œ ë°œìŒì´ë‚˜ ë¬¸ë§¥ ìƒ ìì—°ìŠ¤ëŸ½ì§€ ì•Šì€ í‘œí˜„ì„ GPTë¥¼ í†µí•´ í›„ì²˜ë¦¬ êµì •
            corrected = self.openai_client.create_response(
                system_content = (
                    "ë‹¹ì‹ ì€ ë©´ì ‘ìì˜ ìŒì„± ì¸ì‹ ê²°ê³¼ë¥¼ ë§ì¶¤ë²•ê³¼ íë¦„ ìœ„ì£¼ë¡œ êµì •í•˜ëŠ” êµì • ë„ìš°ë¯¸ì…ë‹ˆë‹¤. \
                    ì‚¬ìš©ìì˜ ì–´íˆ¬ ë° ì–´ë¯¸ë¥¼ ë³´ì¡´í•˜ê³ , ë¬¸ë²• ì˜¤ë¥˜ì™€ ì• ë’¤ ë‹¨ì–´ì™€ ì´ì–´ì§€ì§€ ì•ŠëŠ” ì–´ìƒ‰í•œ í‘œí˜„ë§Œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •í•˜ì„¸ìš”. ì˜ˆ: 'êµë³¸ ê·¼ë¬´' â†’ 'êµë²ˆê·¼ë¬´' í˜¹ì€ 'êµëŒ€ê·¼ë¬´' \
                    ì˜¤íƒ€ë‚˜ ì˜ëª» ì¸ì‹ëœ ë‹¨ì–´ëŠ” ë¬¸ë§¥ìƒ ìì—°ìŠ¤ëŸ½ê²Œ ë³µì›í•˜ì„¸ìš”. ì˜ˆ: 'ì´í›„' â†’ 'ì´ìœ '. \
                    ë‹µë³€ì€ ë¬¸ì¥ í•˜ë‚˜ë¡œ ì¶œë ¥í•˜ë˜, ë©´ì ‘ ë§íˆ¬(ìì—°ìŠ¤ëŸ½ê³  ê³µì†í•œ êµ¬ì–´ì²´)ë¥¼ ìœ ì§€í•˜ë©° ë¬¸ì¥ êµ¬ì¡°ë¥¼ ë°”ê¾¸ì§€ ë§ê³  \
                    í‘œí˜„ë§Œ ìœ„ì— ì œì‹œí•œëŒ€ë¡œ ë‹¤ë“¬ìœ¼ì„¸ìš”."
                ),
                user_content = text
            )

            # êµì • í›„ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ì €ì¥
            original_embedding = OpenAIEmbedding(text)
            corrected_embedding = OpenAIEmbedding(corrected)

            similarity = cosine_similarity([original_embedding], [corrected_embedding])[0][0]

            # 2. ê°ì • ë¶„ì„
            audio_features = extract_audio_features(audio_dir, start, end) # ìŒí–¥ì  íŠ¹ì§• ì¶”ì¶œ

            emotion_prompt = f"""
            ë¬¸ì¥: "{text}"
            í‰ê·  pitch: {audio_features["pitch_mean"]:.1f}Hz, pitch ë³€í™”ëŸ‰: {audio_features["pitch_std"]:.2f}
            í‰ê·  ì—ë„ˆì§€: {audio_features["energy_mean"]:.5f}, ì—ë„ˆì§€ ë³€í™”ëŸ‰: {audio_features["energy_std"]:.5f}
            ë§ ë¹ ë¥´ê¸°(WPS): {wps:.2f}
            """

            emotion = self.openai_client.create_response(
                system_content = (
                    "ë‹¹ì‹ ì€ ë¬¸ì¥ì˜ ê°ì •ì„ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
                    "í…ìŠ¤íŠ¸ì™€ ìŒí–¥ì  íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì´ ë¬¸ì¥ì˜ ê°ì •ì„ ì¶”ë¡ í•˜ì„¸ìš”: "
                    "ê°ì • í•˜ë‚˜ì˜ ë‹¨ì–´ë§Œ ì¶œë ¥í•˜ì„¸ìš”."
                ),
                user_content = emotion_prompt
            )

            print(f"[{start:.1f}s --> {end:.1f}s] [{emotion}] {corrected}")

            # ë¶„ì„ ëœ ë¬¸ì¥ íŒŒì¼ ì €ì¥ì„ ìœ„í•´ append
            sentences.append({
                "original_sentence": text,
                "corrected_sentence": corrected,
                "cosine_similarity": similarity,
                "emotion": emotion,
                "start": start,
                "end": end,
                "wps": wps
            })

        # JSON ë°ì´í„° êµ¬ì„±
        output_data = {"interview": sentences, "LUFS": loudness}
        base_filename = os.path.splitext(os.path.basename(audio_dir))[0]

        dump_json(filename = base_filename, json_data = output_data)

class AudioEvaluator:
    def __init__(self, filename):
        self.filename = filename
        self.data = read_json(filename = filename)
    
    def evaluate(self):
        # ë¬¸ì¥ë³„ í‰ê°€
        similarities = []
        for idx, sentence in enumerate(self.data.get("interview", []), 1):
            similarity = sentence.get("cosine_similarity")
            emotion = sentence.get("emotion")
            wps = sentence.get("wps")
            
            similarities.append(similarity)
            if similarity < 0.8:
                print(f"âŒ ë¬¸ì¥ {idx}: ì˜ë¯¸ì  ì°¨ì´ í¼ (ìœ ì‚¬ë„ {similarity:.4f}) â€” êµì • í•„ìš”")
            elif similarity < 0.9:
                print(f"âš ï¸ ë¬¸ì¥ {idx}: ì•½ê°„ì˜ ì°¨ì´ ìˆìŒ (ìœ ì‚¬ë„ {similarity:.4f}) â€” ìì—°ìŠ¤ëŸ¬ì›€ ê°œì„ ")
            else:
                print(f"âœ… ë¬¸ì¥ {idx}: ê±°ì˜ ë™ì¼ (ìœ ì‚¬ë„ {similarity:.4f}) â€” ë¬¸ë²• êµì •ë§Œ í•„ìš”")

            print(f"ë¬¸ì¥ {idx}ì˜ ê°ì • : {emotion}")

            if wps is None:
                print(f"ë¬¸ì¥ {idx}: WPS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            elif 1.73 <= wps <= 2.59:
                print(f"âœ… ë¬¸ì¥ {idx}ì˜ WPS [{wps}] : ì •ìƒ")
            else:
                print(f"âš ï¸ ë¬¸ì¥ {idx}ì˜ WPS [{wps}] : ë¹ ë¥´ê±°ë‚˜ ëŠë¦½ë‹ˆë‹¤.")
    
        # ì „ì²´ í‰ê°€
        lufs = self.data.get("LUFS")
        if lufs is None:
            print("LUFS ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        elif -24 <= lufs <= -22:
            print(f"âœ… ëª©ì†Œë¦¬ í‰ê·  í¬ê¸° [{lufs} LUFS] : ì •ìƒ")
        else:
            print(f"âš ï¸ ëª©ì†Œë¦¬ í‰ê·  í¬ê¸° [{lufs} LUFS] : ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìŠµë‹ˆë‹¤. ")
        
        if similarities:
            avg_sim = sum(similarities) / len(similarities)
            print(f"\nğŸ“Š ì „ì²´ í‰ê·  ì½”ì‚¬ì¸ ìœ ì‚¬ë„: {avg_sim:.4f}")